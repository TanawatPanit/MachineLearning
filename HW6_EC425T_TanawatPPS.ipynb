{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be8f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanawat Panitpongsri UID 7060636642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787818b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW 6 ECON425T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f19cd5",
   "metadata": {},
   "source": [
    "# Problem 1 (ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0a1fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.outliers_influence as smo\n",
    "import patsy as pt\n",
    "import statsmodels.stats.api as sms\n",
    "from simple_colors import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import truncnorm\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.optimize import minimize_scalar\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, auc\n",
    "from sklearn import tree  \n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import TensorDataset\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d31c6dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights for one-layer NN: [0.99999486 1.        ]\n",
      "Training error for one-layer NN: 6.27678327139154e-13\n",
      "Optimized weights for two-layer NN: [5.59245113 3.46517549 0.67279059]\n",
      "Training error for two-layer NN: 0.010079973099081412\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000000\n",
    "X = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "y = sigmoid(X)\n",
    "\n",
    "# Construct 1-layer NN\n",
    "def f_nn1(weights, X):\n",
    "    w0, w1 = weights\n",
    "    h1 = sigmoid(w0 * X)\n",
    "    y_hat_nn1 = w1 * h1\n",
    "    return y_hat_nn1\n",
    "\n",
    "# Construct 2-layer NN\n",
    "def f_nn2(weights, X):\n",
    "    w0, w1, w2 = weights\n",
    "    h1 = sigmoid(w0 * X)\n",
    "    h2 = sigmoid(w1 * h1)\n",
    "    y_hat_nn2 = w2 * h2\n",
    "    return y_hat_nn2\n",
    "\n",
    "# Define the average squared loss function\n",
    "def average_squared_loss(weights, X, y, nn_model):\n",
    "    y_pred = nn_model(weights, X)\n",
    "    return np.mean((y_pred - y) ** 2)\n",
    "\n",
    "# Define a function for optimization using scipy.optimize.minimize\n",
    "def optimize_nn(nn_model, X, y, initial_weights):\n",
    "    result = minimize(fun=average_squared_loss, x0=initial_weights, args=(X, y, nn_model))\n",
    "    return result.x, result.fun\n",
    "\n",
    "# Initialize weights for a one-layer NN\n",
    "initial_weights_nn1 = np.array([0.0, 0.0])\n",
    "\n",
    "# Fit the one-layer NN\n",
    "optimized_weights_nn1, training_error_nn1 = optimize_nn(f_nn1, X, y, initial_weights_nn1)\n",
    "\n",
    "# Print the optimized weight and training error for the single-layer NN\n",
    "print(\"Optimized weights for one-layer NN:\", optimized_weights_nn1)\n",
    "print(\"Training error for one-layer NN:\", training_error_nn1)\n",
    "\n",
    "# Initialize weights for the two-layer NN\n",
    "initial_weights_nn2 = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "# Fit the two-layer NN\n",
    "optimized_weights_nn2, training_error_nn2 = optimize_nn(f_nn2, X, y, initial_weights_nn2)\n",
    "\n",
    "# Print the optimized weights and training error for the two-layer NN\n",
    "print(\"Optimized weights for two-layer NN:\", optimized_weights_nn2)\n",
    "print(\"Training error for two-layer NN:\", training_error_nn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36697fa7",
   "metadata": {},
   "source": [
    "## (ii)\n",
    "\n",
    "From one-layer NN, I get w0 and w1 value closing to 1 and get training error close to 0.\n",
    "On the other hand, I get higher training error from two-layer NN compared to the one-layer NN. I also get w0 = 5.59245113, w1 = 3.46517549, w2 = 0.67279059.\n",
    "The two-layer NN is too complicated for the true relationship of y and x. This makes trainging error of two-layer NN is significant higher than training error of the one-layer NN, which is similar to the true relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d417516",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5575231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"card_transdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1fdfffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"fraud\"]].copy()\n",
    "X = df.drop(\"fraud\", axis = 1)\n",
    "\n",
    "categorical_feature_names = ['repeat_retailer', 'used_chip', 'used_pin_number', 'online_order']\n",
    "categorical_features_indices = [X.columns.get_loc(name) for name in categorical_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f21c4967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to numpy arrays\n",
    "X_train_numpy = X[:500000].to_numpy(dtype=np.float32)\n",
    "y_train_numpy = y[:500000].to_numpy(dtype=np.float32).ravel()\n",
    "X_test_numpy = X[500000:].to_numpy(dtype=np.float32)\n",
    "y_test_numpy = y[500000:].to_numpy(dtype=np.float32).ravel()\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train_numpy)\n",
    "y_train_tensor = torch.from_numpy(y_train_numpy).long()  # Ensure target tensor is of type long\n",
    "X_test_tensor = torch.from_numpy(X_test_numpy)\n",
    "y_test_tensor = torch.from_numpy(y_test_numpy).long()\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8019bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X_train_numpy.shape[1], 512),  # Adjust the input features dynamically\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, len(np.unique(y_train_numpy)))  # Adjust the output features dynamically\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the model\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b625c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57af79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{len(dataloader.dataset):>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c385007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73baf254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.734922  [    0/500000]\n",
      "loss: 0.514990  [ 6400/500000]\n",
      "loss: 1.408822  [12800/500000]\n",
      "loss: 0.509477  [19200/500000]\n",
      "loss: 0.652709  [25600/500000]\n",
      "loss: 0.388789  [32000/500000]\n",
      "loss: 0.384087  [38400/500000]\n",
      "loss: 0.387976  [44800/500000]\n",
      "loss: 0.380724  [51200/500000]\n",
      "loss: 0.531155  [57600/500000]\n",
      "loss: 0.265888  [64000/500000]\n",
      "loss: 0.362437  [70400/500000]\n",
      "loss: 0.407665  [76800/500000]\n",
      "loss: 0.287708  [83200/500000]\n",
      "loss: 0.338064  [89600/500000]\n",
      "loss: 0.292732  [96000/500000]\n",
      "loss: 0.142228  [102400/500000]\n",
      "loss: 0.336705  [108800/500000]\n",
      "loss: 0.220611  [115200/500000]\n",
      "loss: 0.240624  [121600/500000]\n",
      "loss: 0.558213  [128000/500000]\n",
      "loss: 0.201503  [134400/500000]\n",
      "loss: 0.283654  [140800/500000]\n",
      "loss: 0.298639  [147200/500000]\n",
      "loss: 0.310254  [153600/500000]\n",
      "loss: 0.196349  [160000/500000]\n",
      "loss: 0.726584  [166400/500000]\n",
      "loss: 0.192304  [172800/500000]\n",
      "loss: 0.200646  [179200/500000]\n",
      "loss: 0.186326  [185600/500000]\n",
      "loss: 0.275377  [192000/500000]\n",
      "loss: 0.175370  [198400/500000]\n",
      "loss: 0.203458  [204800/500000]\n",
      "loss: 0.220010  [211200/500000]\n",
      "loss: 0.154092  [217600/500000]\n",
      "loss: 0.312276  [224000/500000]\n",
      "loss: 0.326735  [230400/500000]\n",
      "loss: 0.785892  [236800/500000]\n",
      "loss: 0.164973  [243200/500000]\n",
      "loss: 0.500390  [249600/500000]\n",
      "loss: 0.257350  [256000/500000]\n",
      "loss: 0.136074  [262400/500000]\n",
      "loss: 0.339356  [268800/500000]\n",
      "loss: 0.235523  [275200/500000]\n",
      "loss: 0.136566  [281600/500000]\n",
      "loss: 0.214756  [288000/500000]\n",
      "loss: 0.219198  [294400/500000]\n",
      "loss: 0.269791  [300800/500000]\n",
      "loss: 0.099722  [307200/500000]\n",
      "loss: 0.215597  [313600/500000]\n",
      "loss: 0.187199  [320000/500000]\n",
      "loss: 0.187331  [326400/500000]\n",
      "loss: 0.188215  [332800/500000]\n",
      "loss: 0.444353  [339200/500000]\n",
      "loss: 0.193750  [345600/500000]\n",
      "loss: 0.157895  [352000/500000]\n",
      "loss: 0.290784  [358400/500000]\n",
      "loss: 0.192306  [364800/500000]\n",
      "loss: 0.176892  [371200/500000]\n",
      "loss: 0.190624  [377600/500000]\n",
      "loss: 0.271588  [384000/500000]\n",
      "loss: 0.275353  [390400/500000]\n",
      "loss: 0.468501  [396800/500000]\n",
      "loss: 0.161553  [403200/500000]\n",
      "loss: 0.232502  [409600/500000]\n",
      "loss: 0.170585  [416000/500000]\n",
      "loss: 0.286092  [422400/500000]\n",
      "loss: 0.183740  [428800/500000]\n",
      "loss: 0.205966  [435200/500000]\n",
      "loss: 0.202446  [441600/500000]\n",
      "loss: 0.256168  [448000/500000]\n",
      "loss: 0.257279  [454400/500000]\n",
      "loss: 0.202634  [460800/500000]\n",
      "loss: 0.139816  [467200/500000]\n",
      "loss: 0.206642  [473600/500000]\n",
      "loss: 0.195668  [480000/500000]\n",
      "loss: 0.195223  [486400/500000]\n",
      "loss: 0.188936  [492800/500000]\n",
      "loss: 0.145312  [499200/500000]\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.180356 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.241492  [    0/500000]\n",
      "loss: 0.129366  [ 6400/500000]\n",
      "loss: 0.253991  [12800/500000]\n",
      "loss: 0.161237  [19200/500000]\n",
      "loss: 0.184468  [25600/500000]\n",
      "loss: 0.177474  [32000/500000]\n",
      "loss: 0.192569  [38400/500000]\n",
      "loss: 0.125749  [44800/500000]\n",
      "loss: 0.217952  [51200/500000]\n",
      "loss: 0.201472  [57600/500000]\n",
      "loss: 0.137059  [64000/500000]\n",
      "loss: 0.196564  [70400/500000]\n",
      "loss: 0.160410  [76800/500000]\n",
      "loss: 0.167601  [83200/500000]\n",
      "loss: 0.422170  [89600/500000]\n",
      "loss: 0.218470  [96000/500000]\n",
      "loss: 0.067505  [102400/500000]\n",
      "loss: 0.157111  [108800/500000]\n",
      "loss: 0.155160  [115200/500000]\n",
      "loss: 0.157629  [121600/500000]\n",
      "loss: 0.363403  [128000/500000]\n",
      "loss: 0.115045  [134400/500000]\n",
      "loss: 0.210564  [140800/500000]\n",
      "loss: 0.190580  [147200/500000]\n",
      "loss: 0.115952  [153600/500000]\n",
      "loss: 0.136722  [160000/500000]\n",
      "loss: 0.131304  [166400/500000]\n",
      "loss: 0.118137  [172800/500000]\n",
      "loss: 0.142348  [179200/500000]\n",
      "loss: 0.097435  [185600/500000]\n",
      "loss: 0.168319  [192000/500000]\n",
      "loss: 0.073906  [198400/500000]\n",
      "loss: 0.110213  [204800/500000]\n",
      "loss: 0.149219  [211200/500000]\n",
      "loss: 0.190727  [217600/500000]\n",
      "loss: 0.164227  [224000/500000]\n",
      "loss: 0.297152  [230400/500000]\n",
      "loss: 0.214403  [236800/500000]\n",
      "loss: 0.102007  [243200/500000]\n",
      "loss: 0.206495  [249600/500000]\n",
      "loss: 0.150514  [256000/500000]\n",
      "loss: 0.090592  [262400/500000]\n",
      "loss: 0.238613  [268800/500000]\n",
      "loss: 0.179640  [275200/500000]\n",
      "loss: 0.092872  [281600/500000]\n",
      "loss: 0.160917  [288000/500000]\n",
      "loss: 0.203100  [294400/500000]\n",
      "loss: 0.207772  [300800/500000]\n",
      "loss: 0.066072  [307200/500000]\n",
      "loss: 0.127023  [313600/500000]\n",
      "loss: 0.133947  [320000/500000]\n",
      "loss: 0.223611  [326400/500000]\n",
      "loss: 0.115268  [332800/500000]\n",
      "loss: 0.322996  [339200/500000]\n",
      "loss: 0.122625  [345600/500000]\n",
      "loss: 0.127413  [352000/500000]\n",
      "loss: 0.280460  [358400/500000]\n",
      "loss: 0.141084  [364800/500000]\n",
      "loss: 0.128981  [371200/500000]\n",
      "loss: 0.146514  [377600/500000]\n",
      "loss: 0.178390  [384000/500000]\n",
      "loss: 0.201090  [390400/500000]\n",
      "loss: 0.488785  [396800/500000]\n",
      "loss: 0.123087  [403200/500000]\n",
      "loss: 0.185905  [409600/500000]\n",
      "loss: 0.138366  [416000/500000]\n",
      "loss: 0.171012  [422400/500000]\n",
      "loss: 0.148459  [428800/500000]\n",
      "loss: 0.128976  [435200/500000]\n",
      "loss: 0.152910  [441600/500000]\n",
      "loss: 0.196928  [448000/500000]\n",
      "loss: 0.148443  [454400/500000]\n",
      "loss: 0.133129  [460800/500000]\n",
      "loss: 0.064833  [467200/500000]\n",
      "loss: 0.166149  [473600/500000]\n",
      "loss: 0.148148  [480000/500000]\n",
      "loss: 0.133062  [486400/500000]\n",
      "loss: 0.148502  [492800/500000]\n",
      "loss: 0.101637  [499200/500000]\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.140608 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.190651  [    0/500000]\n",
      "loss: 0.085384  [ 6400/500000]\n",
      "loss: 0.154072  [12800/500000]\n",
      "loss: 0.139248  [19200/500000]\n",
      "loss: 0.110190  [25600/500000]\n",
      "loss: 0.137106  [32000/500000]\n",
      "loss: 0.138132  [38400/500000]\n",
      "loss: 0.137504  [44800/500000]\n",
      "loss: 0.190799  [51200/500000]\n",
      "loss: 0.152939  [57600/500000]\n",
      "loss: 0.108627  [64000/500000]\n",
      "loss: 0.157183  [70400/500000]\n",
      "loss: 0.128155  [76800/500000]\n",
      "loss: 0.119495  [83200/500000]\n",
      "loss: 0.276785  [89600/500000]\n",
      "loss: 0.177309  [96000/500000]\n",
      "loss: 0.048016  [102400/500000]\n",
      "loss: 0.158652  [108800/500000]\n",
      "loss: 0.125747  [115200/500000]\n",
      "loss: 0.133229  [121600/500000]\n",
      "loss: 0.307567  [128000/500000]\n",
      "loss: 0.087931  [134400/500000]\n",
      "loss: 0.175341  [140800/500000]\n",
      "loss: 0.160152  [147200/500000]\n",
      "loss: 0.084982  [153600/500000]\n",
      "loss: 0.108683  [160000/500000]\n",
      "loss: 0.102480  [166400/500000]\n",
      "loss: 0.097382  [172800/500000]\n",
      "loss: 0.116656  [179200/500000]\n",
      "loss: 0.075969  [185600/500000]\n",
      "loss: 0.139245  [192000/500000]\n",
      "loss: 0.054961  [198400/500000]\n",
      "loss: 0.086560  [204800/500000]\n",
      "loss: 0.125509  [211200/500000]\n",
      "loss: 0.161254  [217600/500000]\n",
      "loss: 0.137163  [224000/500000]\n",
      "loss: 0.239770  [230400/500000]\n",
      "loss: 0.163740  [236800/500000]\n",
      "loss: 0.080878  [243200/500000]\n",
      "loss: 0.169815  [249600/500000]\n",
      "loss: 0.124515  [256000/500000]\n",
      "loss: 0.070423  [262400/500000]\n",
      "loss: 0.205821  [268800/500000]\n",
      "loss: 0.154584  [275200/500000]\n",
      "loss: 0.074234  [281600/500000]\n",
      "loss: 0.130911  [288000/500000]\n",
      "loss: 0.141733  [294400/500000]\n",
      "loss: 0.161306  [300800/500000]\n",
      "loss: 0.066840  [307200/500000]\n",
      "loss: 0.105124  [313600/500000]\n",
      "loss: 0.110082  [320000/500000]\n",
      "loss: 0.218587  [326400/500000]\n",
      "loss: 0.090863  [332800/500000]\n",
      "loss: 0.237501  [339200/500000]\n",
      "loss: 0.114872  [345600/500000]\n",
      "loss: 0.096756  [352000/500000]\n",
      "loss: 0.252300  [358400/500000]\n",
      "loss: 0.106551  [364800/500000]\n",
      "loss: 0.104887  [371200/500000]\n",
      "loss: 0.124096  [377600/500000]\n",
      "loss: 0.158516  [384000/500000]\n",
      "loss: 0.164748  [390400/500000]\n",
      "loss: 0.242914  [396800/500000]\n",
      "loss: 0.099044  [403200/500000]\n",
      "loss: 0.161079  [409600/500000]\n",
      "loss: 0.118787  [416000/500000]\n",
      "loss: 0.142690  [422400/500000]\n",
      "loss: 0.122050  [428800/500000]\n",
      "loss: 0.103291  [435200/500000]\n",
      "loss: 0.119412  [441600/500000]\n",
      "loss: 0.146216  [448000/500000]\n",
      "loss: 0.130846  [454400/500000]\n",
      "loss: 0.108402  [460800/500000]\n",
      "loss: 0.062654  [467200/500000]\n",
      "loss: 0.171030  [473600/500000]\n",
      "loss: 0.132289  [480000/500000]\n",
      "loss: 0.104703  [486400/500000]\n",
      "loss: 0.129887  [492800/500000]\n",
      "loss: 0.081101  [499200/500000]\n",
      "Test Error: \n",
      " Accuracy: 95.7%, Avg loss: 0.116972 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.156444  [    0/500000]\n",
      "loss: 0.056090  [ 6400/500000]\n",
      "loss: 0.122942  [12800/500000]\n",
      "loss: 0.116210  [19200/500000]\n",
      "loss: 0.099546  [25600/500000]\n",
      "loss: 0.117561  [32000/500000]\n",
      "loss: 0.112278  [38400/500000]\n",
      "loss: 0.107106  [44800/500000]\n",
      "loss: 0.169610  [51200/500000]\n",
      "loss: 0.133062  [57600/500000]\n",
      "loss: 0.096107  [64000/500000]\n",
      "loss: 0.130096  [70400/500000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.111932  [76800/500000]\n",
      "loss: 0.107608  [83200/500000]\n",
      "loss: 0.282060  [89600/500000]\n",
      "loss: 0.140874  [96000/500000]\n",
      "loss: 0.035154  [102400/500000]\n",
      "loss: 0.129785  [108800/500000]\n",
      "loss: 0.119486  [115200/500000]\n",
      "loss: 0.126072  [121600/500000]\n",
      "loss: 0.306278  [128000/500000]\n",
      "loss: 0.074841  [134400/500000]\n",
      "loss: 0.156402  [140800/500000]\n",
      "loss: 0.137816  [147200/500000]\n",
      "loss: 0.066412  [153600/500000]\n",
      "loss: 0.100588  [160000/500000]\n",
      "loss: 0.086298  [166400/500000]\n",
      "loss: 0.082366  [172800/500000]\n",
      "loss: 0.088186  [179200/500000]\n",
      "loss: 0.065993  [185600/500000]\n",
      "loss: 0.153835  [192000/500000]\n",
      "loss: 0.046290  [198400/500000]\n",
      "loss: 0.070381  [204800/500000]\n",
      "loss: 0.104790  [211200/500000]\n",
      "loss: 0.158993  [217600/500000]\n",
      "loss: 0.122310  [224000/500000]\n",
      "loss: 0.191072  [230400/500000]\n",
      "loss: 0.133120  [236800/500000]\n",
      "loss: 0.060792  [243200/500000]\n",
      "loss: 0.147721  [249600/500000]\n",
      "loss: 0.123170  [256000/500000]\n",
      "loss: 0.044291  [262400/500000]\n",
      "loss: 0.269368  [268800/500000]\n",
      "loss: 0.130622  [275200/500000]\n",
      "loss: 0.063975  [281600/500000]\n",
      "loss: 0.131341  [288000/500000]\n",
      "loss: 0.094903  [294400/500000]\n",
      "loss: 0.150217  [300800/500000]\n",
      "loss: 0.060228  [307200/500000]\n",
      "loss: 0.094305  [313600/500000]\n",
      "loss: 0.093644  [320000/500000]\n",
      "loss: 0.103528  [326400/500000]\n",
      "loss: 0.074757  [332800/500000]\n",
      "loss: 0.137646  [339200/500000]\n",
      "loss: 0.102701  [345600/500000]\n",
      "loss: 0.078694  [352000/500000]\n",
      "loss: 0.229394  [358400/500000]\n",
      "loss: 0.077446  [364800/500000]\n",
      "loss: 0.088107  [371200/500000]\n",
      "loss: 0.097180  [377600/500000]\n",
      "loss: 0.131225  [384000/500000]\n",
      "loss: 0.151118  [390400/500000]\n",
      "loss: 0.458203  [396800/500000]\n",
      "loss: 0.081620  [403200/500000]\n",
      "loss: 0.143386  [409600/500000]\n",
      "loss: 0.101543  [416000/500000]\n",
      "loss: 0.131298  [422400/500000]\n",
      "loss: 0.093589  [428800/500000]\n",
      "loss: 0.077759  [435200/500000]\n",
      "loss: 0.095522  [441600/500000]\n",
      "loss: 0.108791  [448000/500000]\n",
      "loss: 0.123508  [454400/500000]\n",
      "loss: 0.082433  [460800/500000]\n",
      "loss: 0.053054  [467200/500000]\n",
      "loss: 0.145664  [473600/500000]\n",
      "loss: 0.101226  [480000/500000]\n",
      "loss: 0.086107  [486400/500000]\n",
      "loss: 0.122835  [492800/500000]\n",
      "loss: 0.056811  [499200/500000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.099971 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.127736  [    0/500000]\n",
      "loss: 0.045808  [ 6400/500000]\n",
      "loss: 0.105545  [12800/500000]\n",
      "loss: 0.102074  [19200/500000]\n",
      "loss: 0.083854  [25600/500000]\n",
      "loss: 0.090962  [32000/500000]\n",
      "loss: 0.096764  [38400/500000]\n",
      "loss: 0.099135  [44800/500000]\n",
      "loss: 0.127646  [51200/500000]\n",
      "loss: 0.124785  [57600/500000]\n",
      "loss: 0.076221  [64000/500000]\n",
      "loss: 0.121908  [70400/500000]\n",
      "loss: 0.105152  [76800/500000]\n",
      "loss: 0.104340  [83200/500000]\n",
      "loss: 0.245971  [89600/500000]\n",
      "loss: 0.112508  [96000/500000]\n",
      "loss: 0.028925  [102400/500000]\n",
      "loss: 0.111888  [108800/500000]\n",
      "loss: 0.100303  [115200/500000]\n",
      "loss: 0.112205  [121600/500000]\n",
      "loss: 0.312773  [128000/500000]\n",
      "loss: 0.065201  [134400/500000]\n",
      "loss: 0.140378  [140800/500000]\n",
      "loss: 0.122160  [147200/500000]\n",
      "loss: 0.062902  [153600/500000]\n",
      "loss: 0.087565  [160000/500000]\n",
      "loss: 0.111000  [166400/500000]\n",
      "loss: 0.073804  [172800/500000]\n",
      "loss: 0.085098  [179200/500000]\n",
      "loss: 0.057498  [185600/500000]\n",
      "loss: 0.106238  [192000/500000]\n",
      "loss: 0.036479  [198400/500000]\n",
      "loss: 0.057302  [204800/500000]\n",
      "loss: 0.081126  [211200/500000]\n",
      "loss: 0.133377  [217600/500000]\n",
      "loss: 0.095363  [224000/500000]\n",
      "loss: 0.163921  [230400/500000]\n",
      "loss: 0.106995  [236800/500000]\n",
      "loss: 0.046119  [243200/500000]\n",
      "loss: 0.131705  [249600/500000]\n",
      "loss: 0.084281  [256000/500000]\n",
      "loss: 0.033186  [262400/500000]\n",
      "loss: 0.119777  [268800/500000]\n",
      "loss: 0.103085  [275200/500000]\n",
      "loss: 0.049914  [281600/500000]\n",
      "loss: 0.097756  [288000/500000]\n",
      "loss: 0.082159  [294400/500000]\n",
      "loss: 0.136041  [300800/500000]\n",
      "loss: 0.041558  [307200/500000]\n",
      "loss: 0.128036  [313600/500000]\n",
      "loss: 0.082194  [320000/500000]\n",
      "loss: 0.101005  [326400/500000]\n",
      "loss: 0.075017  [332800/500000]\n",
      "loss: 0.101943  [339200/500000]\n",
      "loss: 0.079704  [345600/500000]\n",
      "loss: 0.069816  [352000/500000]\n",
      "loss: 0.184872  [358400/500000]\n",
      "loss: 0.060263  [364800/500000]\n",
      "loss: 0.075095  [371200/500000]\n",
      "loss: 0.084750  [377600/500000]\n",
      "loss: 0.118045  [384000/500000]\n",
      "loss: 0.132409  [390400/500000]\n",
      "loss: 0.151529  [396800/500000]\n",
      "loss: 0.086956  [403200/500000]\n",
      "loss: 0.183855  [409600/500000]\n",
      "loss: 0.094408  [416000/500000]\n",
      "loss: 0.124679  [422400/500000]\n",
      "loss: 0.074239  [428800/500000]\n",
      "loss: 0.066141  [435200/500000]\n",
      "loss: 0.096280  [441600/500000]\n",
      "loss: 0.093397  [448000/500000]\n",
      "loss: 0.118143  [454400/500000]\n",
      "loss: 0.064457  [460800/500000]\n",
      "loss: 0.055220  [467200/500000]\n",
      "loss: 0.174521  [473600/500000]\n",
      "loss: 0.095372  [480000/500000]\n",
      "loss: 0.073893  [486400/500000]\n",
      "loss: 0.121212  [492800/500000]\n",
      "loss: 0.050634  [499200/500000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.089095 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Run training and testing\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6140b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9652\n",
      "Test Precision: 0.8801\n",
      "Test Recall: 0.6967\n",
      "Test F1: 0.7777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def test(dataloader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())  # Append predicted labels\n",
    "            all_labels.extend(y.cpu().numpy())  # Append true labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(all_labels, all_preds, average='binary')\n",
    "    recall = recall_score(all_labels, all_preds, average='binary')\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)  # Calculate accuracy\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1: {f1:.4f}\")\n",
    "\n",
    "test(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850d6f2",
   "metadata": {},
   "source": [
    "From the trained model performance, I get the final model with accuracy = 96.5% improving from 92.3% from the first round of iterations while the average loss decreses from 0.1803 to 0.0891 in the final iteration.\n",
    "However, the simple decision tree from Homework5 outperform this NN model since the decision tree model has Recall = 99.97%, Precision = 100%, and F1 = 99.99%. The NN model has Test Recall = 69.67%, Precision = 88.01%, and F1 = 77.77%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18110575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
